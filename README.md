This project focuses on data preprocessing and cleaning of the Titanic dataset as part of EV LAB Task 1.
It includes end-to-end steps such as data exploration, feature engineering, missing value imputation, outlier detection, and feature scaling.
The workflow applies both traditional and machine-learning-based techniques (like KNN imputation and Isolation Forest) to produce a clean, standardized dataset ready for analysis and modeling.
